{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('../')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('Work/chelombus/data/10M_ZINC_id_Sim_mqn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Work/chelombus/data/10M_ZINC_id_Sim_mqn.csv'\n",
    "\n",
    "\n",
    "s.split('.')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_handler import DataHandler\n",
    "data_handler = DataHandler()\n",
    "smiles_list, features = data_handler.extract_smiles_and_features(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fingerprint_calculator import FingerprintCalculator\n",
    "fp_calculator = FingerprintCalculator()\n",
    "\n",
    "fingerprints = fp_calculator.calculate_fingerprints(smiles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import rdMolDescriptors    \n",
    "from rdkit import Chem\n",
    "\n",
    "m = Chem.MolFromSmiles('CO[C@@H]1[C@@H](OC(N)=O)[C@@H](O)[C@H](Oc2ccc3c(O)c(NC(=O)c4ccc(O)c(CC=C(C)C)c4)c(=O)oc3c2C)OC1(C)C')\n",
    "\n",
    "ds = rdMolDescriptors.MQNs_(m)                                                                    \n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def estimate_components(data_sample):\n",
    "    # Assume data_sample is a NumPy array of shape (n_samples, n_features)\n",
    "    \n",
    "    # Step 2: Fit PCA without reducing dimensions\n",
    "    pca = PCA(n_components=len(fingerprints[1]))\n",
    "    pca.fit(data_sample)\n",
    "    \n",
    "    # Step 3: Calculate cumulative explained variance\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Step 4: Determine the number of components to retain desired variance\n",
    "    threshold = 0.95  # For 95% variance\n",
    "    n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "    print(f\"Number of components to retain {threshold*100}% of variance: {n_components}\")\n",
    "    \n",
    "    # Optional: Plot cumulative explained variance\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Explained Variance vs. Number of Components')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return n_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_components(fingerprints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good, should look into other FP that can reduce the number of dimensions and still conserve the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop(columns= ['target_id', 'standard_type', 'target_name', 'standard_relation', 'standard_value', 'standard_units', 'target_organism'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('data/cleaned_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "from math import sqrt\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return sqrt(sum((a - b) ** 2 for a, b in zip(p1, p2)))\n",
    "\n",
    "def find_most_similar_points(filename):\n",
    "    points = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            points.append((float(row['x']), float(row['y']), float(row['z'])))\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    most_similar = None\n",
    "\n",
    "    for combo in itertools.combinations(range(len(points)), 10):\n",
    "        p1, p2, p3 = [points[i] for i in combo]\n",
    "        dist = (euclidean_distance(p1, p2) + \n",
    "                euclidean_distance(p2, p3) + \n",
    "                euclidean_distance(p3, p1))\n",
    "        if dist < min_distance:\n",
    "            min_distance = dist\n",
    "            most_similar = (p1, p2, p3)\n",
    "\n",
    "    return most_similar\n",
    "\n",
    "# Usage\n",
    "filename = '/home/afloresep/work/chelombus/data/output/batch_data_0.csv'\n",
    "result = find_most_similar_points(filename)\n",
    "print(\"The 3 most similar points are:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proves that same molecules are given the same coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from collections import defaultdict\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return sqrt(sum((a - b) ** 2 for a, b in zip(p1, p2)))\n",
    "\n",
    "def find_most_similar_points(filename):\n",
    "    points = defaultdict(list)\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            point = (float(row['x']), float(row['y']), float(row['z']))\n",
    "            points[point].append(i)\n",
    "\n",
    "    unique_points = list(points.keys())\n",
    "    \n",
    "    if len(unique_points) < 3:\n",
    "        return \"Not enough unique points to find 3 most similar.\"\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    most_similar = None\n",
    "\n",
    "    for combo in itertools.combinations(range(len(unique_points)), 3):\n",
    "        p1, p2, p3 = [unique_points[i] for i in combo]\n",
    "        dist = (euclidean_distance(p1, p2) + \n",
    "                euclidean_distance(p2, p3) + \n",
    "                euclidean_distance(p3, p1))\n",
    "        if dist < min_distance:\n",
    "            min_distance = dist\n",
    "            most_similar = (p1, p2, p3)\n",
    "\n",
    "    return most_similar, min_distance\n",
    "\n",
    "# Usage\n",
    "filename = '/home/afloresep/work/chelombus/data/output/batch_data_0.csv'\n",
    "result, distance = find_most_similar_points(filename)\n",
    "print(\"The 3 most similar non-duplicate points are:\", result)\n",
    "print(\"Total distance between these points:\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return sqrt(sum((a - b) ** 2 for a, b in zip(p1, p2)))\n",
    "\n",
    "def find_most_similar_points(filename, n=10):\n",
    "    points = defaultdict(list)\n",
    "    smiles_dict = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            point = (float(row['x']), float(row['y']), float(row['z']))\n",
    "            points[point].append(i)\n",
    "            smiles_dict[point] = row['smiles']\n",
    "\n",
    "    unique_points = list(points.keys())\n",
    "    \n",
    "    if len(unique_points) < n:\n",
    "        return f\"Not enough unique points to find {n} most similar.\"\n",
    "\n",
    "    distances = []\n",
    "    for combo in itertools.combinations(range(len(unique_points)), n):\n",
    "        combo_points = [unique_points[i] for i in combo]\n",
    "        dist = sum(euclidean_distance(p1, p2) \n",
    "                   for p1, p2 in itertools.combinations(combo_points, 2))\n",
    "        distances.append((dist, combo_points))\n",
    "\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    return distances[0]\n",
    "\n",
    "def plot_smiles(smiles_list):\n",
    "    mols = [Chem.MolFromSmiles(smile) for smile in smiles_list]\n",
    "    \n",
    "    for mol in mols:\n",
    "        AllChem.Compute2DCoords(mol)\n",
    "    \n",
    "    rows = (len(smiles_list) + 4) // 5  # 5 molecules per row\n",
    "    fig, axes = plt.subplots(rows, 5, figsize=(20, 4*rows))\n",
    "    fig.suptitle(\"2D Structures of 10 Most Similar Molecules\", fontsize=16)\n",
    "\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes\n",
    "\n",
    "    for i, (mol, ax) in enumerate(zip(mols, axes)):\n",
    "        img = Draw.MolToImage(mol)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Structure {i+1}\", fontsize=10)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "filename = '/home/afloresep/work/chelombus/data/output/batch_data_19531.csv'\n",
    "result, distance = find_most_similar_points(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler()\n",
    "\n",
    "data_handler.find_input_type(file_path='test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Load all fingerprints with tqdm progress bar\n",
    "fingerprints, smiles_list, features = [], [], []\n",
    "for idx in tqdm(range(75), desc=\"Loading Fingerprints\"):\n",
    "    with open(f'data/fingerprints_chunk_{idx}.pkl', 'rb') as f:\n",
    "        fps_chunk, smiles_chunk, features_chunk = pickle.load(f)\n",
    "        fingerprints.extend(fps_chunk)\n",
    "        smiles_list.extend(smiles_chunk)\n",
    "        features.extend(features_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def plot_smiles(smiles_list):\n",
    "    mols = [Chem.MolFromSmiles(smile) for smile in smiles_list]\n",
    "    \n",
    "    # Generate 2D coordinates for each molecule\n",
    "    for mol in mols:\n",
    "        AllChem.Compute2DCoords(mol)\n",
    "    \n",
    "    # Create a 1x3 subplot\n",
    "    fig, axes = plt.subplots(1, 6, figsize=(15, 5))\n",
    "    fig.suptitle(\"2D Structures of SMILES\", fontsize=16)\n",
    "\n",
    "    for i, (mol, ax) in enumerate(zip(mols, axes)):\n",
    "        img = Draw.MolToImage(mol)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Structure {i+1}\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# SMILES strings\n",
    "smiles = ['Cc1cc(C(=O)N2CC[C@H]2CN(C)C(=O)C(C)(C)C(F)F)c(C)[nH]1','Cc1oc(C(C)C)cc1C(=O)N(C)C[C@@H]1CCN1C(=O)C(C)(F)F',\n",
    "'CCCc1[nH]ccc1C(=O)N(C)C[C@@H]1CCN1C(=O)CC(F)(F)F',\n",
    "'CCc1[nH]ccc1C(=O)N1CC[C@H]1CN(C)C(=O)C(C)(C)C(F)F',\n",
    "'Cc1nc[nH]c1C(=O)N(C)C[C@H]1CCN1C(=O)C(C)(C)C(C)(F)F',\n",
    "'Cc1[nH]nc(C(=O)N(C)C[C@@H]2CCN2C(=O)C(C)(C)C(F)F)c1C',\n",
    "'Cc1nn(C(C)C)cc1C(=O)N(C)C[C@@H]1CCN1C(=O)C(C)(F)F']\n",
    "\n",
    "plot_smiles(smiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/afloresep/work/chelombus/data/10M_ZINC_id_Sim_mqn.csv', nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit import Chem\n",
    "\n",
    "file_path = '/home/afloresep/work/chelombus/data/Enamine_REAL_HAC_29_38_1.3B_Part_2_CXSMILES.cxsmiles'\n",
    "\n",
    "cxsmiles_batch = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        pass\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_chunks(file_path, chunksize):\n",
    "    \"\"\" Calculate number of chunks based on chunksize. For tqdm \"\"\"\n",
    "    total_lines = sum(1 for _ in open(file_path)) - 1  # Subtract 1 for header\n",
    "    total_chunks = (total_lines + chunksize - 1) // chunksize\n",
    "    return total_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_total_chunks(file_path, chunksize=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('../')\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "from config import DATA_FILE_PATH, CHUNKSIZE\n",
    "from tqdm import tqdm \n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from src.data_handler import DataHandler, get_total_chunks\n",
    "from src.fingerprint_calculator import FingerprintCalculator\n",
    "from src.output_generator import OutputGenerator\n",
    "\n",
    "# Initialize classes\n",
    "data_handler = DataHandler(DATA_FILE_PATH, CHUNKSIZE)\n",
    "output_gen = OutputGenerator()\n",
    "fp_calculator = FingerprintCalculator()\n",
    "# Load data in chunks\n",
    "start = time.time()\n",
    "data_chunks, total_chunks = data_handler.load_data()\n",
    "# Process chunks with tqdm progress bar\n",
    "num_chunks = 0\n",
    "for idx, chunk in enumerate(tqdm(data_chunks, total=18295, desc=\"Processing Chunks\")):\n",
    "    num_chunks += 1\n",
    "    chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Enamine_REAL_HAC_29_38_1.3B_Part_2_CXSMILES.cxsmiles', 'r', encoding='utf-8') as f:\n",
    "            while True:\n",
    "                smiles = []\n",
    "                features = [] \n",
    "                for _ in range(100):\n",
    "                        line = f.readline().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Enamine_REAL_HAC_29_38_1.3B_Part_2_CXSMILES.cxsmiles', 'r', encoding='utf-8') as f:\n",
    "    for _ in range (1000):\n",
    "        smiles_entry = line.strip().split('\\t')[1:]\n",
    "        print(smiles_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "list = np.array(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CCOC(=O)C1=C(C)N=C(CNC(C2=CC(C)=CC=C2)C2=NN=NN2C(C)(C)C)O1',\n",
       "       'CCOC(=O)C1=C(C)N=C(CNC(C2=CC(C)=CC=C2)C2=NN=NN2C2COC2)O1',\n",
       "       'CCOC(=O)C1=C(C)N=C(CNC(C2=CC(C)=CC=C2)C2=NN=NN2CC2CC2)O1', ...,\n",
       "       'CCOC(=O)C1=C(C)N=C(CNC(=O)N2CCOC(CN(C)C3=CC=CN=N3)C2)O1',\n",
       "       'CCOC(=O)C1=C(C)N=C(CNC(=O)N2CCC(N(C)CC3=NC=C(CC)O3)C2)O1',\n",
       "       'CCOC(=O)C1=C(C)N=C(CNC(=O)C2=NOC(COC3=C(Cl)C=CC=C3Cl)=C2)O1'],\n",
       "      dtype='<U72')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = [item[0] for item in list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lines = sum(1 for _ in open('/home/afloresep/work/chelombus/data/Enamine_REAL_HAC_29_38_1.3B_Part_2_CXSMILES.cxsmiles')) - 1  # Subtract 1 for header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "664075400"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_lines\n",
    "664075400"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
