{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('../')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('Work/chelombus/data/10M_ZINC_id_Sim_mqn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Work/chelombus/data/10M_ZINC_id_Sim_mqn.csv'\n",
    "\n",
    "\n",
    "s.split('.')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_handler import DataHandler\n",
    "data_handler = DataHandler()\n",
    "smiles_list, features = data_handler.extract_smiles_and_features(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fingerprint_calculator import FingerprintCalculator\n",
    "fp_calculator = FingerprintCalculator()\n",
    "\n",
    "fingerprints = fp_calculator.calculate_fingerprints(smiles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import rdMolDescriptors    \n",
    "from rdkit import Chem\n",
    "\n",
    "m = Chem.MolFromSmiles('CO[C@@H]1[C@@H](OC(N)=O)[C@@H](O)[C@H](Oc2ccc3c(O)c(NC(=O)c4ccc(O)c(CC=C(C)C)c4)c(=O)oc3c2C)OC1(C)C')\n",
    "\n",
    "ds = rdMolDescriptors.MQNs_(m)                                                                    \n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def estimate_components(data_sample):\n",
    "    # Assume data_sample is a NumPy array of shape (n_samples, n_features)\n",
    "    \n",
    "    # Step 2: Fit PCA without reducing dimensions\n",
    "    pca = PCA(n_components=len(fingerprints[1]))\n",
    "    pca.fit(data_sample)\n",
    "    \n",
    "    # Step 3: Calculate cumulative explained variance\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Step 4: Determine the number of components to retain desired variance\n",
    "    threshold = 0.95  # For 95% variance\n",
    "    n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "    print(f\"Number of components to retain {threshold*100}% of variance: {n_components}\")\n",
    "    \n",
    "    # Optional: Plot cumulative explained variance\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Explained Variance vs. Number of Components')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return n_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_components(fingerprints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good, should look into other FP that can reduce the number of dimensions and still conserve the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop(columns= ['target_id', 'standard_type', 'target_name', 'standard_relation', 'standard_value', 'standard_units', 'target_organism'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('data/cleaned_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "from math import sqrt\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return sqrt(sum((a - b) ** 2 for a, b in zip(p1, p2)))\n",
    "\n",
    "def find_most_similar_points(filename):\n",
    "    points = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            points.append((float(row['x']), float(row['y']), float(row['z'])))\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    most_similar = None\n",
    "\n",
    "    for combo in itertools.combinations(range(len(points)), 10):\n",
    "        p1, p2, p3 = [points[i] for i in combo]\n",
    "        dist = (euclidean_distance(p1, p2) + \n",
    "                euclidean_distance(p2, p3) + \n",
    "                euclidean_distance(p3, p1))\n",
    "        if dist < min_distance:\n",
    "            min_distance = dist\n",
    "            most_similar = (p1, p2, p3)\n",
    "\n",
    "    return most_similar\n",
    "\n",
    "# Usage\n",
    "filename = '/home/afloresep/work/chelombus/data/output/batch_data_0.csv'\n",
    "result = find_most_similar_points(filename)\n",
    "print(\"The 3 most similar points are:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proves that same molecules are given the same coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from collections import defaultdict\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return sqrt(sum((a - b) ** 2 for a, b in zip(p1, p2)))\n",
    "\n",
    "def find_most_similar_points(filename):\n",
    "    points = defaultdict(list)\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            point = (float(row['x']), float(row['y']), float(row['z']))\n",
    "            points[point].append(i)\n",
    "\n",
    "    unique_points = list(points.keys())\n",
    "    \n",
    "    if len(unique_points) < 3:\n",
    "        return \"Not enough unique points to find 3 most similar.\"\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    most_similar = None\n",
    "\n",
    "    for combo in itertools.combinations(range(len(unique_points)), 3):\n",
    "        p1, p2, p3 = [unique_points[i] for i in combo]\n",
    "        dist = (euclidean_distance(p1, p2) + \n",
    "                euclidean_distance(p2, p3) + \n",
    "                euclidean_distance(p3, p1))\n",
    "        if dist < min_distance:\n",
    "            min_distance = dist\n",
    "            most_similar = (p1, p2, p3)\n",
    "\n",
    "    return most_similar, min_distance\n",
    "\n",
    "# Usage\n",
    "filename = '/home/afloresep/work/chelombus/data/output/batch_data_0.csv'\n",
    "result, distance = find_most_similar_points(filename)\n",
    "print(\"The 3 most similar non-duplicate points are:\", result)\n",
    "print(\"Total distance between these points:\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return sqrt(sum((a - b) ** 2 for a, b in zip(p1, p2)))\n",
    "\n",
    "def find_most_similar_points(filename, n=10):\n",
    "    points = defaultdict(list)\n",
    "    smiles_dict = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            point = (float(row['x']), float(row['y']), float(row['z']))\n",
    "            points[point].append(i)\n",
    "            smiles_dict[point] = row['smiles']\n",
    "\n",
    "    unique_points = list(points.keys())\n",
    "    \n",
    "    if len(unique_points) < n:\n",
    "        return f\"Not enough unique points to find {n} most similar.\"\n",
    "\n",
    "    distances = []\n",
    "    for combo in itertools.combinations(range(len(unique_points)), n):\n",
    "        combo_points = [unique_points[i] for i in combo]\n",
    "        dist = sum(euclidean_distance(p1, p2) \n",
    "                   for p1, p2 in itertools.combinations(combo_points, 2))\n",
    "        distances.append((dist, combo_points))\n",
    "\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    return distances[0]\n",
    "\n",
    "def plot_smiles(smiles_list):\n",
    "    mols = [Chem.MolFromSmiles(smile) for smile in smiles_list]\n",
    "    \n",
    "    for mol in mols:\n",
    "        AllChem.Compute2DCoords(mol)\n",
    "    \n",
    "    rows = (len(smiles_list) + 4) // 5  # 5 molecules per row\n",
    "    fig, axes = plt.subplots(rows, 5, figsize=(20, 4*rows))\n",
    "    fig.suptitle(\"2D Structures of 10 Most Similar Molecules\", fontsize=16)\n",
    "\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes\n",
    "\n",
    "    for i, (mol, ax) in enumerate(zip(mols, axes)):\n",
    "        img = Draw.MolToImage(mol)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Structure {i+1}\", fontsize=10)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "filename = '/home/afloresep/work/chelombus/data/output/batch_data_19531.csv'\n",
    "result, distance = find_most_similar_points(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler()\n",
    "\n",
    "data_handler.find_input_type(file_path='test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Load all fingerprints with tqdm progress bar\n",
    "fingerprints, smiles_list, features = [], [], []\n",
    "for idx in tqdm(range(75), desc=\"Loading Fingerprints\"):\n",
    "    with open(f'data/fingerprints_chunk_{idx}.pkl', 'rb') as f:\n",
    "        fps_chunk, smiles_chunk, features_chunk = pickle.load(f)\n",
    "        fingerprints.extend(fps_chunk)\n",
    "        smiles_list.extend(smiles_chunk)\n",
    "        features.extend(features_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def plot_smiles(smiles_list):\n",
    "    mols = [Chem.MolFromSmiles(smile) for smile in smiles_list]\n",
    "    \n",
    "    # Generate 2D coordinates for each molecule\n",
    "    for mol in mols:\n",
    "        AllChem.Compute2DCoords(mol)\n",
    "    \n",
    "    # Create a 1x3 subplot\n",
    "    fig, axes = plt.subplots(1, 6, figsize=(15, 5))\n",
    "    fig.suptitle(\"2D Structures of SMILES\", fontsize=16)\n",
    "\n",
    "    for i, (mol, ax) in enumerate(zip(mols, axes)):\n",
    "        img = Draw.MolToImage(mol)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Structure {i+1}\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# SMILES strings\n",
    "smiles = ['Cc1cc(C(=O)N2CC[C@H]2CN(C)C(=O)C(C)(C)C(F)F)c(C)[nH]1','Cc1oc(C(C)C)cc1C(=O)N(C)C[C@@H]1CCN1C(=O)C(C)(F)F',\n",
    "'CCCc1[nH]ccc1C(=O)N(C)C[C@@H]1CCN1C(=O)CC(F)(F)F',\n",
    "'CCc1[nH]ccc1C(=O)N1CC[C@H]1CN(C)C(=O)C(C)(C)C(F)F',\n",
    "'Cc1nc[nH]c1C(=O)N(C)C[C@H]1CCN1C(=O)C(C)(C)C(C)(F)F',\n",
    "'Cc1[nH]nc(C(=O)N(C)C[C@@H]2CCN2C(=O)C(C)(C)C(F)F)c1C',\n",
    "'Cc1nn(C(C)C)cc1C(=O)N(C)C[C@@H]1CCN1C(=O)C(C)(F)F']\n",
    "\n",
    "plot_smiles(smiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/afloresep/work/chelombus/data/10M_ZINC_id_Sim_mqn.csv', nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit import Chem\n",
    "\n",
    "file_path = '/home/afloresep/work/chelombus/data/Enamine_REAL_HAC_29_38_1.3B_Part_2_CXSMILES.cxsmiles'\n",
    "\n",
    "cxsmiles_batch = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        pass\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_chunks(file_path, chunksize):\n",
    "    \"\"\" Calculate number of chunks based on chunksize. For tqdm \"\"\"\n",
    "    total_lines = sum(1 for _ in open(file_path)) - 1  # Subtract 1 for header\n",
    "    total_chunks = (total_lines + chunksize - 1) // chunksize\n",
    "    return total_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_total_chunks(file_path, chunksize=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('../')\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "from config import DATA_FILE_PATH, CHUNKSIZE\n",
    "from tqdm import tqdm \n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from src.data_handler import DataHandler, get_total_chunks\n",
    "from src.fingerprint_calculator import FingerprintCalculator\n",
    "from src.output_generator import OutputGenerator\n",
    "\n",
    "# Initialize classes\n",
    "data_handler = DataHandler(DATA_FILE_PATH, CHUNKSIZE)\n",
    "output_gen = OutputGenerator()\n",
    "fp_calculator = FingerprintCalculator()\n",
    "# Load data in chunks\n",
    "start = time.time()\n",
    "data_chunks, total_chunks = data_handler.load_data()\n",
    "# Process chunks with tqdm progress bar\n",
    "num_chunks = 0\n",
    "for idx, chunk in enumerate(tqdm(data_chunks, total=18295, desc=\"Processing Chunks\")):\n",
    "    num_chunks += 1\n",
    "    chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Enamine_REAL_HAC_29_38_1.3B_Part_2_CXSMILES.cxsmiles', 'r', encoding='utf-8') as f:\n",
    "            while True:\n",
    "                smiles = []\n",
    "                features = [] \n",
    "                for _ in range(100):\n",
    "                        line = f.readline().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Enamine_REAL_HAC_29_38_1.3B_Part_2_CXSMILES.cxsmiles', 'r', encoding='utf-8') as f:\n",
    "    for _ in range (1000):\n",
    "        smiles_entry = line.strip().split('\\t')[1:]\n",
    "        print(smiles_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "list = np.array(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = [item[0] for item in list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lines = sum(1 for _ in open('/home/afloresep/work/chelombus/data/Enamine_REAL_HAC_29_38_1.3B_Part_2_CXSMILES.cxsmiles')) - 1  # Subtract 1 for header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lines\n",
    "664075400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2],\n",
    "\n",
    "              [1, 1], [2, 1], [3, 2]])\n",
    "\n",
    "ipca = IncrementalPCA(n_components=2, batch_size=3)\n",
    "\n",
    "ipca.fit(X)\n",
    "IncrementalPCA(batch_size=3, n_components=2)\n",
    "\n",
    "coordinates = ipca.transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "smiles_list = ['sdfsd', 'sdfsd', 'asdfasd', 'dfwfasdf', 'asdfasd', 'asdfasda']\n",
    "batch_data = pd.DataFrame({\n",
    "             'smiles': smiles_list })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(coordinates[0])):\n",
    "    batch_data[f'PCA_{i+1}'] = coordinates[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tqdm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading chunks and calculating fingerprints: 100%|██████████| 28/28 [00:01<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing of data took: 0.02111086845397949 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Fingerprints and fitting : 100%|██████████| 28/28 [00:01<00:00, 26.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Dimensionality Reduction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming Data:   0%|          | 0/28 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save_batch() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2244286/1856326766.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total time: {(end - start)/60} minutes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2244286/1856326766.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Output coordiantes ~before clip with Percentiles.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0moutput_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mfingerprints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: save_batch() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "from config import DATA_FILE_PATH, CHUNKSIZE, PCA_N_COMPONENTS\n",
    "from tqdm import tqdm \n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from src.data_handler import DataHandler, get_total_chunks\n",
    "from src.fingerprint_calculator import FingerprintCalculator\n",
    "from src.output_generator import OutputGenerator, get_percentiles\n",
    "import numpy as np \n",
    "from tdigest import TDigest \n",
    "\n",
    "def main():\n",
    "    # Initialize classes\n",
    "    data_handler = DataHandler(DATA_FILE_PATH, 36050)\n",
    "    output_gen = OutputGenerator()\n",
    "    fp_calculator = FingerprintCalculator()\n",
    "    x_digest = TDigest()\n",
    "    y_digest = TDigest()\n",
    "    z_digest = TDigest()\n",
    "\n",
    "    # Load data in chunks\n",
    "    data_chunks, total_chunks = data_handler.load_data()\n",
    "\n",
    "    # Process chunks with tqdm progress bar\n",
    "    start = time.time()\n",
    "\n",
    "    for idx, chunk in enumerate(tqdm(data_chunks, total=total_chunks, desc=\"Loading chunks and calculating fingerprints\")):\n",
    "\n",
    "        # Check if chunk already exists\n",
    "        if os.path.exists(f'data/1M/fp_chunks/fingerprints_chunk_{idx}.pkl'):\n",
    "            continue\n",
    "        \n",
    "        # Extract smiles and features from chunk\n",
    "        smiles_list, features = data_handler.extract_smiles_and_features(chunk)\n",
    "\n",
    "        # Calculate fingerprints with progress bar\n",
    "        fingerprints = fp_calculator.calculate_fingerprints(smiles_list)\n",
    "\n",
    "        # Save  fingerprints\n",
    "        with open(f'data/1M/fp_chunks/fingerprints_chunk_{idx}.pkl', 'wb') as f:\n",
    "            pickle.dump((fingerprints), f)\n",
    "\n",
    "        # Save rest of data\n",
    "        with open(f'data/1M/features_chunks/smiles_features_chunk_{idx}.pkl', 'wb') as f:\n",
    "            pickle.dump((smiles_list, features), f)\n",
    "\n",
    "        del smiles_list, features, fingerprints # Free space\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Preprocessing of data took: {(end-start)/60} minutes\")\n",
    "\n",
    "\n",
    "    # Partial fit using iPCA\n",
    "    ipca = IncrementalPCA(n_components = PCA_N_COMPONENTS) # Dimensions to reduce to\n",
    "    \n",
    "    for idx in tqdm(range(total_chunks), desc=\"Loading Fingerprints and fitting \"):\n",
    "        with open(f'data/1M/fp_chunks/fingerprints_chunk_{idx}.pkl', 'rb') as f:\n",
    "            fingerprints = pickle.load(f)\n",
    "\n",
    "        ipca.partial_fit(fingerprints)\n",
    "        del fingerprints\n",
    "\n",
    "    # Transform Data and Save results\n",
    "    print(\"Performing Dimensionality Reduction...\")\n",
    "\n",
    "    for idx in tqdm(range(total_chunks), desc='Transforming Data'):\n",
    "        # Load fingerprint\n",
    "        with open(f'data/1M/fp_chunks/fingerprints_chunk_{idx}.pkl', 'rb') as f:\n",
    "            fingerprints = pickle.load(f)\n",
    "\n",
    "        # Load smiles and features\n",
    "        with open(f'data/1M/features_chunks/smiles_features_chunk_{idx}.pkl', 'rb') as f:\n",
    "            smiles_list, features = pickle.load(f)\n",
    "\n",
    "        # Get coordinates in np.array(n_smiles, n_pca_dim)\n",
    "        coordinates = ipca.transform(fingerprints)\n",
    "        \n",
    "        # Update digest for every batch\n",
    "        x_digest.batch_update(coordinates[:,0])\n",
    "        y_digest.batch_update(coordinates[:,1])\n",
    "        z_digest.batch_update(coordinates[:,2])\n",
    "\n",
    "        # Output coordiantes ~before clip with Percentiles. \n",
    "        output_gen.save_batch(idx, coordinates, smiles_list, features)\n",
    "    \n",
    "        del fingerprints, coordinates, smiles_list, features\n",
    "    \n",
    "    percentiles = get_percentiles(x_digest,  y_digest, z_digest)\n",
    "\n",
    "    print('Percentiles: ', percentiles)\n",
    "    \n",
    "    # Mapp PCA coordinates to the 100x100x100 dimensional cube\n",
    "    # mapped_coordinates = output_gen.map_to_grid(coordinates, percentiles)\n",
    "\n",
    "    # Output \n",
    "    \n",
    "\n",
    "    # Clean\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    main()\n",
    "    end = time.time()\n",
    "    print(f\"Total time: {(end - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.abspath(os.curdir)\n",
    "os.chdir('chelombus')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
